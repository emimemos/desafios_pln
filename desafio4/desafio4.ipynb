{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566db77-d7e9-4ee8-bb18-0acdbd6b9640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando el dataset...\n",
      "Dataset descargado.\n",
      "Número de pares de conversación: 0\n",
      "Tamaño del vocabulario: 1\n",
      "Cargando embeddings FastText...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import gensim.downloader as api\n",
    "\n",
    "# 1. Cargar el Dataset ConvAI2\n",
    "import os\n",
    "import requests\n",
    "\n",
    "dataset_url = \"https://huggingface.co/datasets/conv_ai/convai2/raw/main/train.txt\"\n",
    "dataset_path = \"convai2_train.txt\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"Descargando el dataset...\")\n",
    "    response = requests.get(dataset_url)\n",
    "    with open(dataset_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Dataset descargado.\")\n",
    "\n",
    "# Leer y procesar las conversaciones\n",
    "def load_and_process_data(filepath, max_pairs=10000):\n",
    "    input_sentences = []\n",
    "    output_sentences = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 2:\n",
    "                question = clean_text(parts[0])\n",
    "                answer = clean_text(parts[1])\n",
    "                input_sentences.append(question)\n",
    "                output_sentences.append(f\"<sos> {answer} <eos>\")\n",
    "                if len(input_sentences) >= max_pairs:\n",
    "                    break\n",
    "    return input_sentences, output_sentences\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "input_sentences, output_sentences = load_and_process_data(dataset_path, max_pairs=10000)\n",
    "print(\"Número de pares de conversación:\", len(input_sentences))\n",
    "\n",
    "# 2. Tokenización\n",
    "MAX_VOCAB_SIZE = 8000\n",
    "MAX_LEN = 10\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=\"\")\n",
    "tokenizer.fit_on_texts(input_sentences + output_sentences)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Tamaño del vocabulario:\", vocab_size)\n",
    "\n",
    "encoder_input_seq = pad_sequences(tokenizer.texts_to_sequences(input_sentences), maxlen=MAX_LEN, padding='pre')\n",
    "decoder_input_seq = pad_sequences(tokenizer.texts_to_sequences(output_sentences), maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "# Targets (decodificador sin token <sos>)\n",
    "decoder_target_seq = np.zeros_like(decoder_input_seq)\n",
    "decoder_target_seq[:, :-1] = decoder_input_seq[:, 1:]\n",
    "\n",
    "# 3. Preparar Embeddings FastText\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "print(\"Cargando embeddings FastText...\")\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in fasttext_model:\n",
    "        embedding_matrix[idx] = fasttext_model[word]\n",
    "print(\"Embeddings cargados.\")\n",
    "\n",
    "# 4. Construcción del Modelo Seq2Seq\n",
    "n_units = 128\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(MAX_LEN,), name=\"encoder_inputs\")\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(n_units, return_state=True, dropout=0.2)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(MAX_LEN,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)(decoder_inputs)\n",
    "decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True, dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Modelo de entrenamiento\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 5. Entrenamiento del Modelo\n",
    "decoder_targets = np.expand_dims(decoder_target_seq, -1)\n",
    "\n",
    "print(\"Iniciando el entrenamiento...\")\n",
    "model.fit(\n",
    "    [encoder_input_seq, decoder_input_seq],\n",
    "    decoder_targets,\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    validation_split=0.2\n",
    ")\n",
    "print(\"Entrenamiento finalizado.\")\n",
    "\n",
    "# 6. Configuración de Inferencia\n",
    "# Modelo del encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Modelo del decoder para inferencia\n",
    "decoder_state_input_h = Input(shape=(n_units,))\n",
    "decoder_state_input_c = Input(shape=(n_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedding_infer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)\n",
    "decoder_embedding_input = decoder_embedding_infer(decoder_inputs)\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_input, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# 7. Generación de Respuestas\n",
    "def generate_response(input_text):\n",
    "    states_value = encoder_model.predict(pad_sequences(tokenizer.texts_to_sequences([input_text]), maxlen=MAX_LEN))\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['<sos>']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer.index_word.get(sampled_token_index, \"\")\n",
    "        \n",
    "        if sampled_word == \"<eos>\" or len(decoded_sentence.split()) >= MAX_LEN:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += \" \" + sampled_word\n",
    "        \n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "# Prueba del modelo\n",
    "print(\"QA Bot:\", generate_response(\"do you have any pets\"))\n",
    "print(\"QA Bot:\", generate_response(\"where are you from\"))\n",
    "print(\"QA Bot:\", generate_response(\"what is your name\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f1e86-a8d3-43c4-9bdd-a6193ede6b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
